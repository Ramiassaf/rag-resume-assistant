{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e76531",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6db5edd3-9225-41e3-ba80-d2987f93601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from langchain_community .document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c3b479",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f37c3c06-15c7-4ef6-b1e4-1a4d0c617952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Data_dir = Path(\"/workspace/DATA\") \n",
    "assert Data_dir.exists(), f\"Folder not found: {Data_dir}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c49bef",
   "metadata": {},
   "source": [
    "# Document Loading Phase in Rag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcbd0234-3a75-49ce-a5b1-4b2e78d5f753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9 documents\n",
      "Sample text: Lina Alami\n",
      "MLOps Engineer â€” Amman, Jordan\n",
      "Email: lina.alami3@example.com | Phone: +971597257890\n",
      "Professional Summary\n",
      "Experienced MLOps Engineer with a strong track record of building production-grade machine learning\n",
      "systems, from data engineering and model training to deployment and monitoring. Ski\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "for pdf in Data_dir.glob(\"*.pdf\"):\n",
    "    loader = PyMuPDFLoader(str(pdf))\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents\")\n",
    "print(\"Sample text:\" , docs[0].page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d53035b",
   "metadata": {},
   "source": [
    "# Text Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17332988-adad-43a3-a9a7-21d1222b191e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 42 chunks\n",
      "Sample chunk: Lina Alami\n",
      "MLOps Engineer â€” Amman, Jordan\n",
      "Email: lina.alami3@example.com | Phone: +971597257890\n",
      "Professional Summary\n",
      "Experienced MLOps Engineer with a strong track record of building production-grade machine learning\n",
      "systems, from data engineering and model training to deployment and monitoring. Ski\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, # maximum size of each chunk\n",
    "    chunk_overlap=50 # overlap so contecxt is maintained\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(docs)\n",
    "print(f\"Split into {len(chunks)} chunks\")\n",
    "print(\"Sample chunk:\" , chunks[0].page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa77cf8",
   "metadata": {},
   "source": [
    "we do here texxt chunking because the documents are too large to be processed in one go by the model. By breaking them into smaller, manageable chunks, we can ensure that the model can effectively analyze and retrieve relevant information from each part of the document. This approach enhances the model's ability to understand and respond accurately to queries related to the content of the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0322793",
   "metadata": {},
   "source": [
    "# Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78702467-212b-4441-9ad6-8a920987d360",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings= HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# create vector store\n",
    "db = Chroma.from_documents(\n",
    "    documents = chunks,\n",
    "    embedding = embeddings,\n",
    "    collection_name = \"resumes\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d29a8b",
   "metadata": {},
   "source": [
    "Each text chunk is transformed into vector embeddings using the `all-MiniLM-L6-v2` model from Sentence Transformers.  \n",
    "These embeddings capture semantic meaning and are stored in a Chroma vector database under the collection `resumes`.  \n",
    "This enables efficient similarity search, allowing the assistant to retrieve the most relevant resume sections for any query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8f5890",
   "metadata": {},
   "source": [
    "# Semantic Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96d598e0-13fb-404a-9b80-4769299f52b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] {'author': '(anonymous)', 'creationDate': \"D:20250930175541+00'00'\", 'creator': '(unspecified)', 'file_path': '/workspace/DATA/ai_resume_realistic_01_Khalid_Nassar.pdf', 'format': 'PDF 1.4', 'keywords': '', 'modDate': \"D:20250930175541+00'00'\", 'page': 0, 'producer': 'ReportLab PDF Library - www.reportlab.com', 'source': '/workspace/DATA/ai_resume_realistic_01_Khalid_Nassar.pdf', 'subject': '(unspecified)', 'title': '(anonymous)', 'total_pages': 1, 'trapped': ''}\n",
      "post-processing to extract structured candidate data.\n",
      "â€¢ Optimized model inference using TensorRT and mixed precision; achieved 2.5x throughput improvement\n",
      "on GPU.\n",
      "Education\n",
      "MSc in Computer Science, Machine Learning Track â€” University of Technology, 2019\n",
      "Certifications\n",
      "â€¢ AWS Certified Machine Learnin\n",
      "\n",
      "[2] {'author': '(anonymous)', 'creationDate': \"D:20250930175541+00'00'\", 'creator': '(unspecified)', 'file_path': '/workspace/DATA/ai_resume_realistic_05_Sara_Karim.pdf', 'format': 'PDF 1.4', 'keywords': '', 'modDate': \"D:20250930175541+00'00'\", 'page': 1, 'producer': 'ReportLab PDF Library - www.reportlab.com', 'source': '/workspace/DATA/ai_resume_realistic_05_Sara_Karim.pdf', 'subject': '(unspecified)', 'title': '(anonymous)', 'total_pages': 2, 'trapped': ''}\n",
      "Education\n",
      "BSc in Computer Engineering â€” University of Science, 2016\n",
      "Certifications\n",
      "â€¢ TensorFlow Developer Certificate\n",
      "â€¢ AWS Certified Machine Learning â€“ Specialty\n",
      "Languages\n",
      "English (Fluent), Arabic (Native)\n",
      "Generated sample resume â€” 2025-09-30\n"
     ]
    }
   ],
   "source": [
    "query = \"Find candidates with Tensorflow and AWS experience\"\n",
    "results = db.similarity_search(query, k=2)\n",
    "\n",
    "for i, r in enumerate(results, start=1):\n",
    "    print(f\"\\n[{i}] {r.metadata}\")\n",
    "    print(r.page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b2999c",
   "metadata": {},
   "source": [
    "The user query is embedded and compared against all stored resume embeddings using cosine similarity.  \n",
    "Chroma returns the most semantically similar chunks (k=2), allowing the system to find relevant resumes even if they use different wording."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1315cd",
   "metadata": {},
   "source": [
    "#Language Model Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8d0a6ca-9b79-4275-ba62-504b8efaa111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842bee3064444995bd3f8c60f325eb4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_name = \"microsoft/phi-2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=300,\n",
    "    temperature=0.2,\n",
    "    device=0   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685c8909",
   "metadata": {},
   "source": [
    "The Microsoft Phi-2 model is loaded using Hugging Face Transformers for text generation.  \n",
    "Its tokenizer and model handle converting user queries and retrieved context into coherent, natural-language responses.  \n",
    "This LLM forms the â€œGenerationâ€ part of the RAG pipeline, enabling the assistant to answer questions based on resume data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4784b700",
   "metadata": {},
   "source": [
    "# Connecting Chroma Retrieval with the Phi-2 Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba158e3e-bd20-41c7-a6f3-ad109489d449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_info(context_text, user_query):\n",
    "    prompt = f\"\"\"\n",
    "You are an AI recruiter assistant.\n",
    "\n",
    "Using ONLY the context below, answer the user query.\n",
    "Output MUST be formatted exactly like this:\n",
    "Candidate: <candidate placeholder or role if name missing>\n",
    "Skills/Certifications: <comma-separated list>\n",
    "Education: <if mentioned>\n",
    "If nothing matches, say \"I don't know\".\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {user_query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    result = generator(prompt, max_new_tokens=200, temperature=0.1)\n",
    "    return result[0][\"generated_text\"].split(\"Answer:\")[-1].strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c216015e",
   "metadata": {},
   "source": [
    "In this step, we connect the retrieval and generation components of our RAG pipeline.\n",
    "ChromaDB is used to retrieve the most relevant resume chunks that match the userâ€™s query through semantic similarity search.\n",
    "The retrieved text is then combined into a single context_text, which is passed â€” along with the userâ€™s query to the Phi-2 language model via the extract_candidate_info() function.\n",
    "This function uses a structured prompt to guide the model in generating clear, formatted answers (e.g., candidate name, skills, and education)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c412c8",
   "metadata": {},
   "source": [
    "# Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf09de2-281a-41ed-9ada-600c96df7681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "IMPORTANT: You are using gradio version 4.16.0, however version 4.44.1 is available, please upgrade.\n",
      "--------\n",
      "Running on public URL: https://8d68f130d176ce8358.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://8d68f130d176ce8358.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# Connect your Phi-2 model\n",
    "llm = HuggingFacePipeline(pipeline=generator)\n",
    "\n",
    "# Chat Handler \n",
    "def chat_with_assistant(message, history):\n",
    "    retrieved_docs = db.similarity_search(message, k=3)\n",
    "    context_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    answer = extract_candidate_info(context_text, message)\n",
    "    formatted_answer = (\n",
    "        f\"**{answer}**\" if answer else \"ðŸ¤– I couldn't find matching information.\"\n",
    "    )\n",
    "    history.append((message, formatted_answer))\n",
    "    return history\n",
    "\n",
    "#  Gradio UI \n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"## ðŸ§  RAG Resume Assistant (Phi-2)\")\n",
    "    gr.Markdown(\n",
    "        \"Chat with your AI assistant to find the best AI Engineer candidates. \"\n",
    "        \"Ask for specific skills, frameworks, or experiences â€” the assistant retrieves data from your resumes.\"\n",
    "    )\n",
    "\n",
    "    chatbot = gr.Chatbot(label=\"ðŸ’¬ Chat with Resume Assistant\")\n",
    "    user_input = gr.Textbox(\n",
    "        placeholder=\"Example: Give me one candidate with AWS and TensorFlow experience\",\n",
    "        label=\"Your Question\",\n",
    "        lines=1\n",
    "    )\n",
    "    clear = gr.Button(\"Clear Chat\")\n",
    "\n",
    "    #  Event bindings (inside the block)\n",
    "    user_input.submit(chat_with_assistant, [user_input, chatbot], chatbot)\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "# Launch once\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edc180f",
   "metadata": {},
   "source": [
    "Done By: Rami Assaf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_resume",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
